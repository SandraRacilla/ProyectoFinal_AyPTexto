{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8xS3jEpo5Bf"
   },
   "source": [
    "# Minería de tópicos\n",
    "\n",
    "El propósito de esta sección es obtener tópicos de los tweets recabados acerca del Asalto al Capitolio de los Estados Unidos.  \n",
    "Basado en el artículo *Python for NLP: Topic Modeling* por Usman Malik https://stackabuse.com/python-for-nlp-topic-modeling/.  \n",
    "  \n",
    "### Importación de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9CBcQMnop13k"
   },
   "outputs": [],
   "source": [
    "# La mayoría de bibliotecas viene integrada en Anaconda\n",
    "# Para la biblioteca de Google Search, usar el siguiente paquete\n",
    "# python3 -m pip install googlesearch-python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvO65IluvF35"
   },
   "source": [
    "\n",
    "### Función \"Interprete\" de Tópicos\n",
    "\n",
    "\n",
    "Busca en Google las palabras de cada tópico para obtener el título del primer resultado.\n",
    "\n",
    "El cual es posteriormente asigna al tópico como referencia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def googler(busqueda):\n",
    "    http = urllib3.PoolManager()\n",
    "    response = search(busqueda, num_results=10)\n",
    "    #Fetch\n",
    "    for result in response:\n",
    "        result = http.request('GET', result)\n",
    "        soup = BeautifulSoup(result.data,'html.parser')\n",
    "        if type(soup.title) != type(None):\n",
    "            return soup.title.string\n",
    "    return soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvO65IluvF35"
   },
   "source": [
    "\n",
    "### Función Minería de Tópicos Non-Negative Matrix Factorization\n",
    "\n",
    "\n",
    "Antes aplicar el algoritmo de NMF es necesario obtener el vocabulario del archivo\n",
    "\n",
    "NMF hace uso de TFIDF\n",
    "\n",
    "max_df=0.80 -> Palabras que aparezcan al menos en 80% del documento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qBf5BFb9v4aS"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Función que aplica Non-Negative Matrix Factorización al contenido de los tweets.\n",
    "Almacena en un archivo un resumen con las palabras top de cada tópico.\n",
    "@param   reviews_datasets_NMF  dataframe que contiene una columna 'data__text', la cual se refiere al texto\n",
    "                               extraido del tweet\n",
    "@return  reviews_datasets_NMF  dataframe original con una columna extra que denota el tópico al que pertenece\n",
    "\"\"\"\n",
    "def txt_NMF(reviews_datasets_NMF):\n",
    "    \"\"\"\n",
    "    Antes aplicar el algoritmo de NMF es necesario obtener el vocabulario del archivo\n",
    "    NMF hace uso de TFIDF\n",
    "    max_df=0.80 -> Palabras que aparezcan al menos en 80% del documento\n",
    "    min_df=2 -> Palabras que aparezcan al menos en 2 documentos\n",
    "    \"\"\"\n",
    "    my_stop_words=text.ENGLISH_STOP_WORDS.union([\"https\"],[\"nhttps\"],[\"d4leu57x7h\"])\n",
    "    tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words=my_stop_words)\n",
    "    ##Matriz generada con TFIDF\n",
    "    doc_term_matrix = tfidf_vect.fit_transform(reviews_datasets_NMF['data__text'].values.astype('U'))\n",
    "    \"\"\"\n",
    "    Uso de NMF para crear temas junto con la distribución de probabilidad para cada palabra del vocabulario\n",
    "    n_components:5 -> Numero de categorias o temas que en las que queremos\n",
    "                        que se divida nuestro texto\n",
    "    random_state:42 -> seed \n",
    "    Creamos una matriz de probabilidad con las probabilidades de todas las palabras en el vocabulario\n",
    "    \"\"\"\n",
    "    nmf = NMF(n_components=5, random_state=42)\n",
    "    nmf.fit(doc_term_matrix )\n",
    "\n",
    "  \n",
    "    \"\"\"\n",
    "    ###Palabras de nuestro vocabulario\n",
    "    for i in range(10):\n",
    "        random_id = random.randint(0,len(tfidf_vect.get_feature_names()))\n",
    "        print(tfidf_vect.get_feature_names()[random_id])\n",
    "    \"\"\"\n",
    "    ##Para encontrar el primer topic se usa \"components_\" con atributo 0\n",
    "    first_topic = nmf.components_[0]\n",
    "    ##first_topic contiene la probabilidad de 3716 palabras para el topic 1\n",
    "    ##Ordenamos los índices de acuerdo a los valores de las probabilidades\n",
    "    ##Regresa indíces de 10 palabras con las probabilidades más altas\n",
    "    top_topic_words = first_topic.argsort()[-10:]\n",
    "    \"\"\"\n",
    "    Pasamos índices al vector para observar las palabras\n",
    "    \n",
    "    for i in top_topic_words:\n",
    "        print(tfidf_vect.get_feature_names()[i])\n",
    "    \"\"\"\n",
    "    fic = open(\"topics_NMF.txt\", \"w\")\n",
    "    print('\\t\\t\\t\\tTemas NMF', file=fic)\n",
    "    for i,topic in enumerate(nmf.components_):\n",
    "        print(f'NMF Top 10 words for topic #{i}:', file=fic)\n",
    "        print(f'NMF Top 10 words for topic #{i}:')\n",
    "        words_tpc = [tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]]\n",
    "        article_title = googler(' '.join(words_tpc))\n",
    "        print(words_tpc, file=fic)\n",
    "        print(words_tpc)\n",
    "        print(article_title, file=fic)\n",
    "        print(article_title)\n",
    "        print('\\n\\n', file=fic)\n",
    "        print('\\n\\n')\n",
    "    fic.close()\n",
    "\n",
    "    topic_values = nmf.transform(doc_term_matrix)\n",
    "    reviews_datasets_NMF['Topic'] = topic_values.argmax(axis=1)\n",
    "    print(reviews_datasets_NMF.head())\n",
    "    return reviews_datasets_NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJiz76FzGAO8"
   },
   "source": [
    "### Función Minería de Tópicos Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XauTlh_6GS4H"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Función que aplica Latent Dirichlet Allocation al contenido de los tweets.\n",
    "Almacena en un archivo un resumen con las palabras top de cada tópico.\n",
    "@param   reviews_datasets_LDA  dataframe que contiene una columna 'data__text', la cual se refiere al texto\n",
    "                               extraido del tweet\n",
    "@return  reviews_datasets_LDA  dataframe original con una columna extra que denota el tópico al que pertenece\n",
    "\"\"\"\n",
    "def txt_LDA(reviews_datasets_LDA):\n",
    "    \"\"\"\n",
    "    Antes aplicar el algoritmo de LDA es necesario obtener el vocabulario\n",
    "    del archivo\n",
    "    max_df=0.80 -> Palabras que aparezcan al menos en 80% del documento\n",
    "    min_df=2 -> Palabras que aparezcan al menos en 2 documentos\n",
    "    \"\"\"\n",
    "    my_stop_words=text.ENGLISH_STOP_WORDS.union([\"https\"],[\"nhttps\"],[\"d4leu57x7h\"],[\"amp\"])\n",
    "    count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words=my_stop_words)\n",
    "    doc_term_matrix = count_vect.fit_transform(reviews_datasets_LDA['data__text'].values.astype('U'))\n",
    "\n",
    "    \"\"\"\n",
    "    Uso de LDA para crear temas junto con la distribución de probabilidad \n",
    "    para cada palabra del vocabulario\n",
    "    n_components:5 -> Numero de categorias o temas que en las que queremos\n",
    "                        que se divida nuestro texto\n",
    "    random_state:42 -> seed \n",
    "    \"\"\"\n",
    "    LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    LDA.fit(doc_term_matrix)\n",
    "    \"\"\"\n",
    "    for i in range(10):\n",
    "        random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "        print(count_vect.get_feature_names()[random_id])\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Encontramos 10 palabras con la probabilidad más alta para los temas\n",
    "    first_topic contiene las probabilidades de 3716 palabras para el tema 1\n",
    "    argsort() Ordenar índices de acuerdo a los valores de probabilidades\n",
    "    [-10:] Toma los últimos 10 valores, es decir los que tienen mayor valor  \n",
    "    \"\"\"\n",
    "    first_topic = LDA.components_[0]\n",
    "    #print(len(first_topic))\n",
    "    top_topic_words = first_topic.argsort()[-10:]\n",
    "    #print(top_topic_words)\n",
    "\n",
    "    \"\"\"\n",
    "    Obtenemos palabras relacionadas con los índices anteriores\n",
    "    \n",
    "    for i in top_topic_words:\n",
    "        print(count_vect.get_feature_names()[i])\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Impresion de 10 palabras con mayor probabilidad de cada uno de los 5 temas \n",
    "    \"\"\"\n",
    "    fic = open(\"topics_LDA.txt\", \"w\")\n",
    "    print('\\t\\t\\t\\tTemas LDA', file=fic)\n",
    "    for i,topic in enumerate(LDA.components_):\n",
    "        words_tpc = [count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]]\n",
    "        article_title = googler(' '.join(words_tpc))\n",
    "        print(f'LDA Top 10 words for topic #{i}:', file=fic)\n",
    "        print(words_tpc, file=fic)\n",
    "        print(article_title, file=fic)\n",
    "        print('\\n\\n', file=fic)\n",
    "        print(f'LDA Top 10 words for topic #{i}:')\n",
    "        print(words_tpc)\n",
    "        print(article_title)\n",
    "        print('\\n\\n')\n",
    "    fic.close()\n",
    "\n",
    "    \"\"\"\n",
    "    Agregamos una columna al archivo donde agreguemos el tema al que pertenece\n",
    "    \"\"\"\n",
    "    topic_values = LDA.transform(doc_term_matrix)\n",
    "    topic_values.shape\n",
    "    reviews_datasets_LDA['Topic'] = topic_values.argmax(axis=1)\n",
    "    print(reviews_datasets_LDA.head())\n",
    "    return reviews_datasets_LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suV1QlQ5Gezd"
   },
   "source": [
    "### Importación de Datos y Análisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "2keoydmoGwl6",
    "outputId": "7405f904-4a32-4f77-8c5f-110e2a228b0b"
   },
   "outputs": [],
   "source": [
    "reviews_datasets = pd.read_csv(r'Tweets Recabados.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2428, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_datasets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El data frame consiste en una tabla de 2428 registros (2428 tweets) con 3 columnas.  \n",
    "Para visualizar las columnas mostramos una parte del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data__id</th>\n",
       "      <th>data__lang</th>\n",
       "      <th>data__text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1347495838839238657</td>\n",
       "      <td>en</td>\n",
       "      <td>@PapaGlider @Jessica26307123 @MontyBoa99 @real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1347495838688440320</td>\n",
       "      <td>en</td>\n",
       "      <td>US Capitol: Police confirms death of officer i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1347495838063284230</td>\n",
       "      <td>en</td>\n",
       "      <td>@HookRocky @NBCNews @NBCNewsTHINK At least we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1347495834833747969</td>\n",
       "      <td>en</td>\n",
       "      <td>Mike Pompeo Says Capitol Riot Proves U.S. Isn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1347495834439606272</td>\n",
       "      <td>en</td>\n",
       "      <td>US Capitol Attack: President Trump Can�t Handl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>1347561137278119936</td>\n",
       "      <td>en</td>\n",
       "      <td>Uh oh. Maybe they should�ve worn masks? \\n\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>1347561136690843649</td>\n",
       "      <td>und</td>\n",
       "      <td>??\\n\\nhttps://t.co/FprykuE9bN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>1347561136242126849</td>\n",
       "      <td>en</td>\n",
       "      <td>Oregon representative allowed protesters into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>1347561136187727873</td>\n",
       "      <td>en</td>\n",
       "      <td>@realDonaldTrump The 5 deaths ARE ALL ON YOUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>1347561136087052288</td>\n",
       "      <td>en</td>\n",
       "      <td>@RobbieBarstool Or the capitol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2428 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 data__id data__lang  \\\n",
       "0     1347495838839238657         en   \n",
       "1     1347495838688440320         en   \n",
       "2     1347495838063284230         en   \n",
       "3     1347495834833747969         en   \n",
       "4     1347495834439606272         en   \n",
       "...                   ...        ...   \n",
       "2423  1347561137278119936         en   \n",
       "2424  1347561136690843649        und   \n",
       "2425  1347561136242126849         en   \n",
       "2426  1347561136187727873         en   \n",
       "2427  1347561136087052288         en   \n",
       "\n",
       "                                             data__text  \n",
       "0     @PapaGlider @Jessica26307123 @MontyBoa99 @real...  \n",
       "1     US Capitol: Police confirms death of officer i...  \n",
       "2     @HookRocky @NBCNews @NBCNewsTHINK At least we ...  \n",
       "3     Mike Pompeo Says Capitol Riot Proves U.S. Isn'...  \n",
       "4     US Capitol Attack: President Trump Can�t Handl...  \n",
       "...                                                 ...  \n",
       "2423  Uh oh. Maybe they should�ve worn masks? \\n\\nI ...  \n",
       "2424                      ??\\n\\nhttps://t.co/FprykuE9bN  \n",
       "2425  Oregon representative allowed protesters into ...  \n",
       "2426  @realDonaldTrump The 5 deaths ARE ALL ON YOUR ...  \n",
       "2427                     @RobbieBarstool Or the capitol  \n",
       "\n",
       "[2428 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos si el dataframe contiene registros vacios.\n",
    "En caso de tenerlos, es necesario eliminarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "2keoydmoGwl6",
    "outputId": "7405f904-4a32-4f77-8c5f-110e2a228b0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_datasets.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que no tiene registros nulos, es posible aplicar los algoritmos de modelado de tópicos LDA y NMF.  \n",
    "Cuando se aplican los algoritmos se imprimen las palabras top de cada tópico en cada algorimo, y son almacenadas en archivos de texto para su consula posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minado de tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "2keoydmoGwl6",
    "outputId": "7405f904-4a32-4f77-8c5f-110e2a228b0b"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3Dnazis%2Bmob%2Briots%2Bdon%2Blaw%2Bjust%2Bpeople%2Battack%2Bpolice%2Btrump%26num%3D11%26hl%3Den&hl=en&q=EgTJjabjGMbbiYEGIhkA8aeDS_Ns17dFTQVXZ468dgAoYDqECZEDMgFy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cc7f61f9a6a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreviews_datasets_LDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_LDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreviews_datasets_NMF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_NMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-85a015a39056>\u001b[0m in \u001b[0;36mtxt_LDA\u001b[0;34m(reviews_datasets_LDA)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mwords_tpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0marticle_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_tpc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'LDA Top 10 words for topic #{i}:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_tpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0facc5ca47e7>\u001b[0m in \u001b[0;36mgoogler\u001b[0;34m(busqueda)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgoogler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusqueda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mhttp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPoolManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusqueda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#Fetch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/googlesearch/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(term, num_results, lang)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/googlesearch/__init__.py\u001b[0m in \u001b[0;36mfetch_results\u001b[0;34m(search_term, number_results, language_code)\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                                               language_code)\n\u001b[1;32m     15\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musr_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3Dnazis%2Bmob%2Briots%2Bdon%2Blaw%2Bjust%2Bpeople%2Battack%2Bpolice%2Btrump%26num%3D11%26hl%3Den&hl=en&q=EgTJjabjGMbbiYEGIhkA8aeDS_Ns17dFTQVXZ468dgAoYDqECZEDMgFy"
     ]
    }
   ],
   "source": [
    "reviews_datasets_LDA = txt_LDA(reviews_datasets)\n",
    "print('\\n\\n\\n\\n')\n",
    "reviews_datasets_NMF = txt_NMF(reviews_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de la ejecución de los algoritmos es el datagrame orginal más la columna Topic, que indica el número de tópico al que se relaciona el tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_datasets_LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_datasets_NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de Posturas\n",
    "El propósito de esta sección es encontrar el mejor modelo para el problema de clasificación de posturas.  \n",
    "Basado en el cuaderno *Multi-class text classification (TFIDF)* de Selene Reyes. Disponible en https://www.kaggle.com/selener/multi-class-text-classification-tfidf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de bibliotecas y datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los tweets para entrenamiento y pruebas se encuentran en el directorio\n",
    "# StanceTweetLabeled. Listamos el directorio para verificar que se encuentran\n",
    "# cargados.\n",
    "import os\n",
    "print(os.listdir(\"./StanceTweetLabeled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando datos\n",
    "df_train = pd.read_csv('./StanceTweetLabeled/train.csv' , engine='python')\n",
    "df_test = pd.read_csv('./StanceTweetLabeled/test.csv' , engine='python')\n",
    "print(\"Forma de df_train:\",df_train.shape)\n",
    "print(\"Forma de df_test:\",df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset original tiene divididos los datos en datos de prueba y datos de entrenamiento. Vamos a combinarlos en uno sólo debido a que posteriormente dividiremos a los datos aplicando validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.append(df_test)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de datos exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impresión de las primeras 2 filas para conocer su estructura\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna \"Target\" se refiere al tópico del tweet.\n",
    "Para conocer cuales son estos podemos obtener sus valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Target\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar para la columna \"Stance\", que se refiere a la postura que la\n",
    " persona toma con respecto a cierto tópico u argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Stance\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las columnas que interesan para el entrenamiento son \"Tweet\" y \"Stance\", pues a a partir de cierto tweet, queremos predecir la postura que viene implicita en el mismo. No consideramos la columna \"Target\" debido a que ninguno de los tópicos del dataset está relacionado con el asunto del Asalto al Capitolio de Estados Unidos 2021.  \n",
    "Para ello se crea un nuevo dataframe con sólo estas dos columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un nuevo dataframe con sólo dos columnas\n",
    "df1 = df[['Stance', 'Tweet']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tipos de posturas son AGAINST, FAVOR, NONE. Los autores del dataset colocan\n",
    "a la postura NONE para aquellos tweets que no tienen que ver con los tópicos que ellos eligieron (Hilary Clinton, Legalization of Abortion, etc), así que para este caso de estudio es mejor omitir todos los tweets clasificados con esta postura.  \n",
    "Primero contamos cuantos tweets hay por cada postura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby('Stance').Tweet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tienen 1256 tweets clasificados como NONE. Se eliminarán todos estos para quedar con sólo tweets de AGAINST y FAVOR.  \n",
    "Para ello los ordenamos por postura y nos quedamos con los primeros 3614 registros, debido a que es lo que se obtiene de restarle el número de tweets NONE al total de tweets. (4870 - 1256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values(by=['Stance']).head(3614)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los diferentes valores de la columna \"Stance\" se tratarán como clases. Se trata entonces de un problema de dos clases. Se necesita representar a cada clase (postura) como números para que el modelo predictivo pueda entenderlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una nueva columna llamada 'stance_id' con las posturas codificadas\n",
    "df1['stance_id'] = df1['Stance'].factorize()[0]\n",
    "stance_id_df = df1[['Stance', 'stance_id']].drop_duplicates()\n",
    "\n",
    "# Diccionarios para mapear entre el identificador de la postura y la postura\n",
    "stance_to_id = dict(stance_id_df.values)\n",
    "id_to_stance = dict(stance_id_df[['stance_id', 'Stance']].values)\n",
    "\n",
    "print(\"Impresión de los diccionarios creados\")\n",
    "print(\"stance_to_id:\",stance_to_id)\n",
    "print(\"id_to_stance:\",id_to_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impresión de un subconjunto aleatorio del nuevo dataframe\n",
    "df1.sample(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un gráfico con el número de de tweets por postura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6),facecolor='white')\n",
    "df1.groupby('Stance').Tweet.count().sort_values().plot.barh(\n",
    "  ylim=0, title= 'Número de posturas por cada tweet')\n",
    "plt.xlabel('Número de ocurrencias', fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento del texto\n",
    "Para este paso se transformarán los tweets a vectores de  $R^n$ para que el modelo pueda realizar predicciones. Para ello se utilizará TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utiliza un escalamiento logaritmico\n",
    "# Se remueven palabras que ocurren en menos de 5 documentos\n",
    "# Se utilizan unigramas y bigramas\n",
    "# Se remueven stop words en inglés\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "# Se transforma cada tweet en un vector de TFIDF\n",
    "features = tfidf.fit_transform(df1.Tweet).toarray()\n",
    "\n",
    "\n",
    "# Se utilizan como etiquetas el identificador de la postura\n",
    "labels = df1.stance_id\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de clasificación\n",
    "En esta sección se entrenan tres modelos, y mediante validación cruzada de 5 iteraciones se obtendrá la exactitud de cada modelo. Aquel que tenga la mejor exactitud será el elegido como modelo para nuestro caso de estudio.  \n",
    "Los modelos que se propone usar son:\n",
    "- Categorical Naive Bayes\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Linear Support Vector Machine\n",
    "- Non Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los modelos son almacenados en una lista\n",
    "models = [\n",
    "  RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "  MultinomialNB(),\n",
    "  CategoricalNB(),\n",
    "  LogisticRegression(random_state=0),\n",
    "  LinearSVC(),\n",
    "  SVC()\n",
    "]\n",
    "\n",
    "# Cross-validation de 5 iteraciones\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "# Se itera sobre la lista de modelos y en la lista entries se almacena\n",
    "# el nombre del modelo y valores obtenidos de la validación cruzada.\n",
    "# De la validación cruzada se almacena el identificador de la iteración y\n",
    "# la exactitud obtenida en cada iteración.\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model,features,labels,scoring='accuracy',cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "# Creamos un nuevo dataframe a partir de la lista entries\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este momento se tiene el dataframe cv_df que contiene la lista de exactitudes obtenidas de la validación cruzada de cada modelo. Se tienen 5 valores distintos de exactitud por cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para decidir que modelo utilizar, se obtiene el promedio y desviación estándar de las exactitudes de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "# Se crea un nuevo dataframe con el promedio y desviación estándar de la\n",
    "# exactitud de cada modelo.\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean_Accuracy', 'Standard_deviation']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6),facecolor='white')\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "plt.title(\"EXACTITUD PROMEDIO (cv = 5)\\n\", size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el diagrama de cajas y vigotes se puede notar que el modelo con mejor exactitud promedio es Multinomial Naive Bayes. Otro aspecto a destacar es que todos los modelos poseen un rendimiento similar, de entre 0.65 y 0.7, así que el elegir entre uno u otro no varía demasiado. Para efectos de este trabajo, nos quedaremos con Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo elegido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se divide los datos en datos de prueba y de entrenamiento\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, \n",
    "                                                                labels, \n",
    "                                                                df1.index, test_size=0.25, \n",
    "                                                                random_state=1)\n",
    "# Se genera un modelo con los datos de entrenamiento obtenidos\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "# Se evalúa con los datos de prueba\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisión, exhaustividad y valor F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\t\\tMÉTRICAS DE CLASIFICACIÓN\\n')\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names= df1.Stance.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matríz de confusión\n",
    "La matriz de confusión es una tabla que representa los valores reales y los valores predecidos. Un buen modelo es aquel que tiene colores oscuros en la diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8),facecolor='white')\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Reds\", fmt='d',\n",
    "            xticklabels=stance_id_df.Stance.values, \n",
    "            yticklabels=stance_id_df.Stance.values)\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predecido')\n",
    "plt.title(\"Matríz de confusión - MultinomialNB\\n\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos observados se puede notar que el clasificador tiende a clasificar las posturas en \"AGAINST\". Esto probablemente se debe a la alta cantidad de registros que tiene esta clase con respecto a las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby('Stance').Tweet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrección del modelo\n",
    "Para tratar de solucionar esta desviación del modelo, eliminaremos 1204 registros clasificados como AGAINST, esto con el objetivo de que la cantidad de registros por cada clase este balanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ordena el dataframe por la columna Stance, lo que coloca los registros con\n",
    "# el orden AGAINST, FAVOR, NONE.\n",
    "# Posteriormente eliminaremos a los primeros 1204 registros, que son parte de\n",
    "# AGAINST. Esto se hace seleccionando los últimos 2410 registros del dataframe.\n",
    "# TOTAL_REGISTROS - REGISTROS A ELIMINAR = REGISTROS A MANTENER\n",
    "# 3614 - 1204 = 2410\n",
    "df_balanced = df1.sort_values(by=['Stance']).tail(2410)\n",
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.groupby('Stance').Tweet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que las clases están balanceadas, repetimos el proceso de preprocesamiento del texto y entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transforma cada tweet en un vector de TFIDF\n",
    "# Notar que ahora se utiliza el dataframe df_balanced\n",
    "features = tfidf.fit_transform(df_balanced.Tweet).toarray()\n",
    "\n",
    "# Se utilizan como etiquetas el identificador de la postura\n",
    "labels = df_balanced.stance_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se divide los datos en datos de prueba y de entrenamiento\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features,\n",
    "                                                                labels,\n",
    "                                                                df_balanced.index, test_size=0.25,\n",
    "                                                                random_state=1)\n",
    "# Se genera un modelo con los datos de entrenamiento obtenidos\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "# Se evalúa con los datos de prueba\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\t\\tMÉTRICAS DE CLASIFICACIÓN\\n')\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names= df1.Stance.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8),facecolor='white')\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Reds\", fmt='d',\n",
    "            xticklabels=stance_id_df.Stance.values, \n",
    "            yticklabels=stance_id_df.Stance.values)\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predecido')\n",
    "plt.title(\"Matríz de confusión - MultinomialNB\\n\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nota que esta vez la matriz de confusión ya tiene un patrón con color obscuro en la diagonal.  \n",
    "Este modelo será el que finalmente utilizaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_balanced['Tweet'] # Datos de entrada\n",
    "y = df_balanced['Stance'] # Variable a predecir\n",
    "\n",
    "# TF-IDF aprende el vocabulario e idf de los datos de entrenamiento\n",
    "# Es decir, se genera un \"Vectorizador\"\n",
    "fitted_vectorizer = tfidf.fit(X)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X)\n",
    "\n",
    "model = MultinomialNB().fit(tfidf_vectorizer_vectors, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la predicción creamos nuevos tweets que reflejen la postura de una persona. A continuación mostramos dos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweet = \"\"\"I don't think in that way\"\"\"\n",
    "print(model.predict(fitted_vectorizer.transform([new_tweet])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweet = \"\"\"She is right\"\"\"\n",
    "print(model.predict(fitted_vectorizer.transform([new_tweet])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de Posturas en Tweets sobre Asalto al Capitolio de Estados Unidos 2021\n",
    "Para esta sección se buscará clasificar las posturas utilizando el modelo generado en la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando el dataframe reviews_datasets_NMF se genera una nueva columna con\n",
    "# el valor de la postura predecido por el modelo Multinomial Naive Bayes\n",
    "tweets_stance_NMF = reviews_datasets_NMF.assign(\n",
    "    Stance=lambda x: ( model.predict(fitted_vectorizer.transform(x['data__text'])))\n",
    ")\n",
    "tweets_stance_NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De forma similar, se genera una nueva columna para reviews_datasets_LDA\n",
    "tweets_stance_LDA = reviews_datasets_LDA.assign(\n",
    "    Stance=lambda x: ( model.predict(fitted_vectorizer.transform(x['data__text'])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener la cantidad de tweets que se agruparon que tuvieron postura AGAINST y FAVOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stance_NMF.groupby('Stance').data__id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual forma la contidad de tweets en cada tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stance_NMF.groupby('Topic').data__id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos también obtener los tweets que se agruparon en AGAINST y FAVOR tomando en cuenta el tópico en el que se agruparon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Conteo de posturas por tópico LDA:\")\n",
    "print(tweets_stance_LDA.groupby(['Topic','Stance']).data__id.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Conteo de posturas por tópico NMF:\")\n",
    "print(tweets_stance_NMF.groupby(['Topic','Stance']).data__id.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación tenemos un gráfico de barras que muestra de forma gráfica el número de posturas por tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6),facecolor='white')\n",
    "tweets_stance_LDA.groupby(['Topic','Stance']).data__id.count().plot.bar(\n",
    "  title= 'Número de posturas por cada tópico de tweet',\n",
    "  color=['green', 'green', 'red', 'red', 'blue', 'blue', \n",
    "        'yellow', 'yellow', 'purple', 'purple']\n",
    ")\n",
    "plt.xlabel('Número de ocurrencias', fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico se nota que el tópico más sobresaliente es el número 1. Además\n",
    "- Para el tópico 0 la mayoría de personas está **en contra**.\n",
    "- Para el tópico 1 la mayoría de personas está **en contra**.\n",
    "- Para el tópico 2 la mayoría de personas está **a favor**.\n",
    "- Para el tópico 3 la mayoría de personas está **en contra**.\n",
    "- Para el tópico 4 la mayoría de personas está **a favor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último exportamos los dataframes finales a csv para su uso individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stance_LDA.to_csv('./OutputData/tweets_stance_LDA.csv', index = False)\n",
    "tweets_stance_NMF.to_csv('./OutputData/tweets_stance_NMF.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Minería de Tópicos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
